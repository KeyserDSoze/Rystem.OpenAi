# Token Usage Tracking Documentation

## Overview

The system now provides **comprehensive token usage tracking** across all OpenAI API calls, including:
- **Input tokens** (prompt)
- **Cached input tokens** (prompt cache hits)
- **Output tokens** (completion)
- **Total tokens** (input + output)

This information is available in each `AiSceneResponse` for detailed monitoring and cost analysis.

## Token Types

### 1. **Input Tokens** (`InputTokens`)
- Tokens used in the prompt/request
- Counted at full rate for pricing
- Includes all user messages, system messages, and function definitions

### 2. **Cached Input Tokens** (`CachedInputTokens`)
- Tokens from the prompt that were cached and reused from previous requests
- Cost-saving feature: charged at **10% of normal input rate**
- Requires prompt caching to be enabled
- Tracked via `ChatUsage.PromptTokensDetails.CachedTokens`

### 3. **Output Tokens** (`OutputTokens`)
- Tokens generated by the model in the response
- Counted at full rate for pricing
- Includes all text, function calls, and JSON responses

### 4. **Total Tokens** (`TotalTokens`)
- Sum of input + output tokens
- Useful for compliance and monitoring

## Model Updates for Cached Tokens Support

### PromptTokensDetails
New model added at `src\Rystem.OpenAi\Endpoints\Chat\Models\Result\PromptTokensDetails.cs`:

```csharp
public sealed class PromptTokensDetails
{
    /// <summary>
    /// Cached tokens from prompt (10% cost reduction)
    /// </summary>
    [JsonPropertyName("cached_tokens")]
    public int? CachedTokens { get; set; }

    /// <summary>
    /// Audio tokens in prompt (for audio inputs)
    /// </summary>
    [JsonPropertyName("audio_tokens")]
    public int? AudioTokens { get; set; }
}
```

### ChatUsage Updated
Modified `ChatUsage` class to include `PromptTokensDetails`:

```csharp
public sealed class ChatUsage
{
    public int PromptTokens { get; set; }
    public int CompletionTokens { get; set; }
    public int TotalTokens { get; set; }
    
    [JsonPropertyName("prompt_tokens_details")]
    public PromptTokensDetails? PromptTokensDetails { get; set; }  // ? NEW
    
    public CompletionTokensDetails? CompletionTokensDetails { get; set; }
}
```

### AiSceneResponse Extended
New properties added to `AiSceneResponse`:

```csharp
public sealed class AiSceneResponse
{
    // ... existing properties ...
    
    /// <summary>
    /// Number of input tokens used in this request
    /// </summary>
    public int? InputTokens { get; set; }

    /// <summary>
    /// Number of cached input tokens reused (10% cost reduction)
    /// </summary>
    public int? CachedInputTokens { get; set; }

    /// <summary>
    /// Number of output tokens generated
    /// </summary>
    public int? OutputTokens { get; set; }

    /// <summary>
    /// Total tokens used (input + output)
    /// </summary>
    public int? TotalTokens { get; set; }
}
```

## Implementation Details

### Token Population Flow

```
ChatResult.Usage (from OpenAI)
    ?
OpenAiChat.AddUsages() - calculates costs
    ?
PopulateTokenCounts() helper method
    ?
AiSceneResponse.{InputTokens, CachedInputTokens, OutputTokens, TotalTokens}
```

### Helper Method

In `SceneManager.cs`:

```csharp
private static void PopulateTokenCounts(AiSceneResponse response, ChatUsage usage)
{
    response.InputTokens = usage.PromptTokens;
    response.CachedInputTokens = usage.PromptTokensDetails?.CachedTokens ?? 0;
    response.OutputTokens = usage.CompletionTokens;
    response.TotalTokens = usage.TotalTokens;
}
```

This method is called whenever we have a `ChatUsage` object to ensure consistent token tracking.

## Usage in Tests

### Accessing Token Counts

```csharp
[Fact]
public async Task TokenTrackingTest()
{
    var responses = await ExecuteTurnAsync("Some question", conversationKey);
    
    // Get response with token info
    var response = responses.FirstOrDefault(r => r.Status == AiResponseStatus.Running);
    
    if (response != null)
    {
        Console.WriteLine($"Input Tokens: {response.InputTokens}");
        Console.WriteLine($"Cached Tokens: {response.CachedInputTokens}");
        Console.WriteLine($"Output Tokens: {response.OutputTokens}");
        Console.WriteLine($"Total Tokens: {response.TotalTokens}");
        
        // Calculate cost with cache benefit
        decimal cachedSavings = response.CachedInputTokens.GetValueOrDefault() * 0.9m;
        Console.WriteLine($"Cache Savings: ${cachedSavings}");
    }
}
```

### Aggregating Tokens Across Conversation

```csharp
[Fact]
public async Task ConversationTokenAggregationTest()
{
    var conversationKey = Guid.NewGuid().ToString();
    
    // Turn 1
    var responses1 = await ExecuteTurnAsync("First question", conversationKey);
    var turn1Tokens = responses1.Sum(r => r.TotalTokens ?? 0);
    
    // Turn 2
    var responses2 = await ExecuteTurnAsync("Second question", conversationKey);
    var turn2Tokens = responses2.Sum(r => r.TotalTokens ?? 0);
    
    var totalTokens = turn1Tokens + turn2Tokens;
    Console.WriteLine($"Total tokens used: {totalTokens}");
    
    // Track cached tokens reuse
    var cachedTokens = responses2.Sum(r => r.CachedInputTokens ?? 0);
    Console.WriteLine($"Cached tokens reused: {cachedTokens}");
}
```

## Cost Calculation with Cached Tokens

### Standard Model Pricing (example: GPT-4)
- Input: $0.03 per 1K tokens
- Output: $0.06 per 1K tokens
- Cached Input: **$0.003 per 1K tokens** (90% savings!)

### Cost Formula

```csharp
decimal CalculateCost(int inputTokens, int cachedTokens, int outputTokens)
{
    const decimal INPUT_RATE = 0.03m / 1000;
    const decimal CACHED_RATE = 0.003m / 1000;  // 10% of normal rate
    const decimal OUTPUT_RATE = 0.06m / 1000;
    
    decimal inputCost = inputTokens * INPUT_RATE;
    decimal cachedCost = cachedTokens * CACHED_RATE;
    decimal outputCost = outputTokens * OUTPUT_RATE;
    
    return inputCost + cachedCost + outputCost;
}
```

## Monitoring Token Usage

### Per-Request Tracking
Each `AiSceneResponse` has:
- Individual token counts
- Per-request cost
- Total accumulated cost

### Multi-Scene Workflow Example

```
Request: "Get weather and book vacation"
??? Scene: Weather
?   ??? FunctionRequest: {InputTokens: 100, OutputTokens: 50}
?   ??? Running: {InputTokens: 150, CachedInputTokens: 10, OutputTokens: 80}
??? Scene: Vacation
?   ??? FunctionRequest: {InputTokens: 120, OutputTokens: 40}
?   ??? Running: {InputTokens: 160, CachedInputTokens: 20, OutputTokens: 90}
??? Total: {InputTokens: 530, CachedInputTokens: 30, OutputTokens: 260, TotalTokens: 820}
```

## Prompt Caching Optimization

To maximize cached token reuse:

1. **Enable prompt caching** in OpenAI API
2. **Reuse conversation key** for multi-turn conversations
3. **Keep system messages consistent** - they get cached
4. **Batch similar requests** - cache warmup is shared

### Example: Multi-Turn Conversation

```csharp
[Fact]
public async Task PromptCachingBenefitTest()
{
    var conversationKey = "user-123-session";  // Reuse same key
    
    // Turn 1: First question (cache created)
    var resp1 = await ExecuteTurnAsync("What's the capital of France?", conversationKey);
    var cached1 = resp1.Sum(r => r.CachedInputTokens ?? 0);
    // cached1 = 0 (no cache yet)
    
    // Turn 2: Follow-up (cache reused!)
    var resp2 = await ExecuteTurnAsync("And what about Germany?", conversationKey);
    var cached2 = resp2.Sum(r => r.CachedInputTokens ?? 0);
    // cached2 > 0 (system messages and context cached!)
    
    Console.WriteLine($"Cache benefit: {cached2} tokens reused at 90% discount");
}
```

## OpenAI Cost Optimization Tips

1. **Use prompt caching** - up to 90% savings on cached tokens
2. **Batch requests** - when possible
3. **Monitor cached token ratio** - should increase over time
4. **Reuse conversations** - better cache reuse
5. **Review token breakdown** - identify optimization opportunities

## Debugging Token Issues

### Unexpected High Token Count?

```csharp
var response = responses.FirstOrDefault();
if (response?.InputTokens > 1000)
{
    Console.WriteLine($"High input tokens: {response.InputTokens}");
    Console.WriteLine($"  - Cached: {response.CachedInputTokens} (should be higher)");
    Console.WriteLine($"Suggestion: Check if conversation key is reused properly");
}
```

### Cache Not Working?

- Verify prompt caching is enabled in OpenAI settings
- Check `CachedInputTokens` - should be > 0 after first turn
- Ensure conversation key is consistent
- System messages must be identical between turns

## Summary

| Metric | Property | Purpose |
|--------|----------|---------|
| Input Tokens | `InputTokens` | Full-price prompt tokens |
| Cached Tokens | `CachedInputTokens` | Reused tokens (90% discount) |
| Output Tokens | `OutputTokens` | Generated response tokens |
| Total | `TotalTokens` | Sum of all tokens |
| Cost | `Cost` | USD cost of this request |
| Total Cost | `TotalCost` | Accumulated cost for conversation |

---

**Related Documentation:**
- [COST_TRACKING.md](./COST_TRACKING.md) - Pricing and cost calculation
- [SCORE_BASED_VALIDATION.md](./SCORE_BASED_VALIDATION.md) - Response validation
- [ChatUsage API Reference](../src/Rystem.OpenAi/Endpoints/Chat/Models/Result/ChatUsage.cs)
